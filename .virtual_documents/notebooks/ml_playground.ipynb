# Import core libraries for data manipulation and visualization
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Import machine learning tools from scikit-learn
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import normalize
from sklearn.dummy import DummyClassifier

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings("ignore")





# Define key configuration parameters for preprocessing and modeling
params = { 
    # path to ML-ready dataset
    'for_ml' : '../processed/ambitus_0_15_ml_ready_29_07_2025.csv',
    # categorical string columns
    'str_cols' : ['Group', 'Sex', 'Season'],
    # columns to exclude from ML
    'not_for_ml' : ['Group', 'Sex', 'Season', 'NR', 'Year', 'Paradigm', 'id', 'Generation'],
    # excluded from correlation
    'not_corr_cols' : ['Group', 'Sex', 'Season', 'NR', 'Year', 'Paradigm', 'id', 'Generation', 'Group'],
    # upper bound for analysis range
    'generation_cut' : 13,
    # lower bound for analysis range
    'generation_under_cut' : 6,
    # random seed for reproducibility
    'seed' : 42
}


#Readading the data
data = pd.read_csv(params['for_ml'])
data





# Perform generation-wise normalization of numeric features
data_normalized = data.copy()
# Loop over each generation separately
for gen in data['Generation'].unique():
    gen_data = data[data['Generation'] == gen]
    # Select numeric columns that are not excluded
    numeric_cols = [col for col in gen_data.columns if col not in params['not_for_ml'] and gen_data[col].dtype in ['float64', 'int64']]
    # Normalize the numeric features within this generation
    data_normalized.loc[data['Generation'] == gen, numeric_cols] = normalize(gen_data[numeric_cols])
# Display the normalized dataset
data_normalized





# Create training and testing sets based on generation thresholds
# Features for training: generations between lower and upper cut
X_train = data.loc[(data['Generation'] <= params['generation_cut']) & (data['Generation'] > params['generation_under_cut'])].drop(columns=params['not_for_ml'])
# Features for testing: generations after the upper cutoff
X_test = data.loc[data['Generation'] > params['generation_cut']].drop(columns=params['not_for_ml'])
# Labels for training and testing (target = Group)
y_train = data.loc[(data['Generation'] <= params['generation_cut']) & (data['Generation'] > params['generation_under_cut']), 'Group']
y_test = data.loc[data['Generation'] > params['generation_cut'], 'Group']





#Plotting label distribution in train and test sets
sns.histplot(pd.DataFrame(y_train), x='Group', multiple='dodge', shrink=0.8)
plt.title('Distribution of Group at Train Set', fontsize=16)
plt.show()






# Create a dummy classifier as a baseline model
dummy = DummyClassifier(strategy='most_frequent', random_state=params['seed'])
dummy.fit(X_train, y_train)

# Predict and evaluate
y_dummy_pred = dummy.predict(X_test)

print("Dummy Classifier – Classification Report:")
print(classification_report(y_test, y_dummy_pred))

# Plot confusion matrix
cm_dummy = confusion_matrix(y_test, y_dummy_pred)
sns.heatmap(cm_dummy, annot=True)
plt.title('Dummy Classifier – Confusion Matrix', fontsize=16)
plt.show()





# Build a simple ML pipeline using a Decision Tree Classifier
pipeline = Pipeline([('tree', DecisionTreeClassifier(random_state=params['seed'], max_depth=5, min_samples_leaf=6))])
# Train the model
pipeline.fit(X_train, y_train)
# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Print classification performance
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Plot the confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True)
plt.title('Confusion Matrix', fontsize=16)
plt.show()





# Plot feature importances from the fitted decision tree model
if hasattr(pipeline.named_steps['tree'], 'feature_importances_'):
    # Extract importance scores
    importances = pipeline.named_steps['tree'].feature_importances_
    feature_names = X_train.columns
    
    # Create a DataFrame for visualization
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    # Sort by importance and filter out features with negligible importance
    importance_df = importance_df.sort_values(by='Importance', ascending=False).loc[importance_df['Importance'] > 0.00001].head(10)  # Filter out low importance features
    # Plot as horizontal bar chart
    sns.barplot(importance_df, x='Importance', y='Feature', orient='h')
    plt.title('Feature Importance', fontsize=16)
    plt.show()

