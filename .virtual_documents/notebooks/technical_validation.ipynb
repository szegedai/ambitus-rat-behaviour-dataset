# Import essential libraries for data handling, visualization, and analysis
import pandas as pd # For data manipulation and analysis
import matplotlib.pyplot as plt # For creating static visualizations
import seaborn as sns # For enhanced statistical data visualization
import numpy as np # For numerical operations

# Import statistical tools
from scipy.stats import spearmanr # For computing Spearman rank correlation
from statsmodels.stats.multitest import multipletests # For multiple testing correction (e.g., FDR)
from statsmodels.formula.api import ols
import statsmodels.api as sm

# Import scikit-learn tool for handling missing values
from sklearn.impute import SimpleImputer # For imputing missing values

# System and warning configuration
import os
import warnings
warnings.filterwarnings('ignore')  # Suppress all warnings
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" # Prevents conflicts in certain parallel computing environments (e.g., on MacOS or when using MKL)


params = {'raw_data': "../raw/ambitus_0_15_log_29_07_2025.parquet",
          'feature_missing':"../results/feature_missingness.csv",
          'feature_missing_fig':"../results/fig_missing_bar.pdf",
          'feature_missing_heatmap':"../results/fig_missing_heatmap.pdf",
          'gen_drift':"../results/gen_drift_spearman.csv",
          'gen_drift_heatmap':"../results/fig_drift_heatmap.pdf",
          'gen_drift_line':"../results/fig_drift_lineplot.pdf"       

      }





# Define the path to the raw data file (Parquet format)
file_path =params['raw_data']
# load the data into a DataFrame
df = pd.read_parquet(file_path)
# Standardize Sex-related columns by converting text to lowercase
df['GR_Sex'] = df['GR_Sex'].str.lower()
df['Sex'] = df['Sex'].str.lower()





# Replace placeholder values (-1) with actual NaNs for proper missing value handling
df_SI = df.replace(-1, np.nan)
# Select all numeric columns from the DataFrame
numeric_cols = df_SI.select_dtypes(include="number").columns
# Keep only numeric columns that are not entirely NaN
numeric_cols = numeric_cols[~df_SI[numeric_cols].isna().all()]

# Initialize a SimpleImputer with the mean strategy
# (You can also use strategies like 'median', 'most_frequent', etc.) 
imputer = SimpleImputer(strategy="mean")   # or 'median', 'most_frequent', etc.
# Apply the imputer to fill missing values in numeric columns
imputed_array = imputer.fit_transform(df_SI[numeric_cols])
# Reconstruct a DataFrame from the imputed array with original indices and column names
df_imputed = pd.DataFrame(imputed_array,
                          columns=numeric_cols,
                          index=df_SI.index)
# Update the original DataFrame with the imputed values
df_SI.loc[:, numeric_cols] = df_imputed


display(df_SI)





# Calculate the percentage of missing values for each column in the full dataset
missing_pct = df.isna().mean().mul(100).sort_values(ascending=False)

# Export the missingness summary to a CSV file (for Supplementary Table S1)
missing_pct.to_csv(params['feature_missing'],
                   index_label="feature",
                   header=["percent_missing"])


display(missing_pct)


missing_pct.head(50)





# Select the 30 features with the highest proportion of missing values
top30 = missing_pct.head(30)
# Create a horizontal bar plot showing the missingness of these top 30 features
plt.figure(figsize=(7, 9))
top30[::-1].plot.barh(color="steelblue") # Plot in reverse order for better readability
# Label the axes and add a title
plt.xlabel("Missing values (%)")
plt.title("Top 30 features by missing rate")
plt.tight_layout()
# Adjust layout and save the figure as a high-resolution PDF
plt.savefig(params['feature_missing_fig'], dpi=300)
# Display the plot
plt.show()
plt.close()





# Create a pivot table: rows = features, columns = generations
# For the top 30 features, compute the percentage of missing values per generation
miss_gen = (
    df.loc[:,list(top30.index)+ ["Generation"]].groupby("Generation")
      .apply(lambda x: x.isna().mean())          # Calculate missing ratio for each feature
      .T                                         # Transpose to make features as rows
      .mul(100)                                  # Convert to percentage
)
# Plot a heatmap of missing values across generations for top 30 features
plt.figure(figsize=(12, 8))
sns.heatmap(
    miss_gen,
    cmap="viridis",
    vmin=0, vmax=100,
    cbar_kws={"label": "Missing (%)"},
    linewidths=0.2
)
# Add labels and formatting
plt.title("Missing-value heatmap by generation(Top 30 features by missing rate)")
plt.xlabel("Generation")
plt.ylabel("Feature")
plt.tight_layout()
# Save the figure as high-resolution PDF
plt.savefig(params['feature_missing_heatmap'], dpi=300)
plt.show()
plt.close()






# Define a list of keywords related to identifiers and non-behavioral metadata
exclude_keywords = ['id', 'group', 'sex', 'trial', 'year', 'gen', 'rat', 'animal']
# Identify columns to exclude if they contain any of the keywords
exclude_columns = [col for col in df.columns if any(key in col.lower() for key in exclude_keywords)]
# Drop the identified non-behavioral columns from the dataset
df_filtered = df.drop(columns=exclude_columns, errors='ignore')

# Keep only numeric columns
df_numeric = df_filtered.select_dtypes(include='number')

# Drop columns with constant values or near-zero variance
df_numeric = df_numeric.loc[:, df_numeric.std() > 0.01]

# Select top 30 variables by standard deviation
top30_columns = df_numeric.std().sort_values(ascending=False).head(30).index
df_top30 = df_numeric[top30_columns]

# Plot the Spearman correlation matrix for the top 30 features
plt.figure(figsize=(8, 6))
sns.heatmap(df_top30.corr(method="spearman"), cmap="coolwarm", center=0, annot=False, fmt=".2f", linewidths=1)
plt.title("Correlation Matrix – Top 30 Behavioral Features", fontsize=16)
# Display the figure
plt.show()





# separate metadata & numeric fesature matrix
meta_cols = ["RatID", "Generation"]                    # Add more metadata columns if needed
all_feat_cols = [c for c in df.columns if c not in meta_cols]

# Select only numeric and boolean columns (booleans are treated as numeric)
numeric_feat_cols = df[all_feat_cols].select_dtypes(
    include=["number", "boolean"]            # bool is OK
).columns.tolist()

# Optionally: print a few of the excluded (non-numeric) columns
dropped = set(all_feat_cols) - set(numeric_feat_cols)
if dropped:
    print(f"{len(dropped)} non-numeric columns skipped: {sorted(dropped)[:5]} ...")

# Compute Spearman correlation between Generation and each numeric feature
results = []
gen = df["Generation"].astype("float") 
for f in numeric_feat_cols:
    rho, p = spearmanr(gen, df[f], nan_policy="omit")
    results.append({"feature": f, "rho": rho, "p": p})
# Compile results and sort by absolute correlation
res = pd.DataFrame(results).sort_values("rho", key=np.abs, ascending=False)
# Apply FDR correction (Benjamini–Hochberg) for multiple testing
res["q"] = multipletests(res["p"], method="fdr_bh")[1]
# Save results to CSV
res.to_csv(params['gen_drift'], index=False)


display(res)





# Select the top 25 features showing the strongest generational drift
top25 = res.head(25)["feature"]
# Compute the Spearman correlation matrix between these top 25 features
# using their median values per generation (to smooth out noise)
corr_sub = (df.groupby("Generation")[top25]
              .median()
              .corr(method="spearman")          # feature × feature, quick normalisation
              .loc[top25, top25])

# Plot the absolute Spearman correlations as a heatmap
plt.figure(figsize=(8, 10))
sns.heatmap(np.abs(corr_sub), cmap="crest", vmin=0, vmax=1,
            cbar_kws={"label": "abs(Spearman ρ)"})
# Add title and formatting
plt.title("Generational drift – top 25 features")
plt.tight_layout()
# Save the figure as a high-resolution PDF
plt.savefig(params['gen_drift_heatmap'], dpi=300)
plt.show()
plt.close()





features_to_plot = [
    "Expl_E_AFT_T",          # latency, ms (large magnitude)
    "Expl_E_AFT_Nr",         # event count (low magnitude)
    "Expl_E_TOT_Loco_ratio"  # dimensionless ratio (very small)
]

df_line = (df.groupby("Generation")[features_to_plot]
             .agg(["median",
                   lambda x: x.quantile(0.25),
                   lambda x: x.quantile(0.75)])
             .rename(columns={"<lambda_0>": "q25", "<lambda_1>": "q75"})
             .reset_index())

df_line.columns = ["Generation"] + [
    f"{f}_{stat}" for f in features_to_plot for stat in ["med", "q25", "q75"]
]


# Create a dual-axis plot to visualize multiple metrics with different scales
fig, ax1 = plt.subplots(figsize=(8, 4.5))
ax2 = ax1.twinx()  # Secondary y-axis

# Assign consistent colors using Matplotlib's default color cycle
col_map = dict(zip(features_to_plot, ["C0", "C1", "C2"]))

# LEFT y-axis: latency feature (typically larger values)
f_left = "Expl_E_AFT_T"
ax1.plot(df_line["Generation"], df_line[f"{f_left}_med"],
         label=f_left, color=col_map[f_left])
ax1.fill_between(df_line["Generation"],
                 df_line[f"{f_left}_q25"],
                 df_line[f"{f_left}_q75"],
                 alpha=0.20, color=col_map[f_left])
ax1.set_ylabel("Latency (ms)")
ax1.set_xlabel("Generation")

# RIGHT y-axis: count and ratio features (typically smaller scale)
for f in ["Expl_E_AFT_Nr", "Expl_E_TOT_Loco_ratio"]:
    ax2.plot(df_line["Generation"], df_line[f"{f}_med"],
             label=f, color=col_map[f])
    ax2.fill_between(df_line["Generation"],
                     df_line[f"{f}_q25"],
                     df_line[f"{f}_q75"],
                     alpha=0.20, color=col_map[f])

ax2.set_ylabel("Count / Ratio")

# Combine legends from both axes
handles, labels = [], []
for ax in (ax1, ax2):
    h, l = ax.get_legend_handles_labels()
    handles.extend(h)
    labels.extend(l)

ax1.legend(handles, labels, frameon=False, loc="upper left")

# Add plot title and finalize layout
ax1.set_title("Generational trend of key behavioural metrics")
plt.tight_layout()
plt.savefig(params['gen_drift_line'], dpi=300)
plt.show()
plt.close()






# Drop rows where grouping variables are missing
df = df.dropna(subset=['GR_Sex', 'Year'])
# Normalize Sex information (lowercase for consistency)
df['Sex'] = df['Sex'].str.lower()
df['Group_Sex'] = df['GR_Sex'].str.lower()

# Define variables to test and corresponding feature names in the dataset
anova_targets = {
    'LOCO_TOT': 'Locomotion (Loco_TOT)',
    'LOCO_BEF': 'Locomotion frequency (LOCO_BEF)',
    'EXPL_TOT': 'Exploration (Expl_TOT)',
    'Expl_E_I_BEF_Nr': 'Exploration frequency (Expl_BEF)',
    'L_C': 'Learning capacity (L_C)',
    'E_E': 'Effective exploration ratio (E_E)'
}

# Initialize a list to collect ANOVA results
anova_results = []

# Perform two-way factorial ANOVA for each target variable
# Including main effects and interactions with Year
for feature, description in anova_targets.items():
    formula = f"{feature} ~ C(Group) + C(Sex) + C(Year) + C(Group):C(Year) + C(Sex):C(Year)"
    model = ols(formula, data=df).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    # Annotate results with feature description and variable name
    anova_table["Feature"] = description
    anova_table["Variable"] = anova_table.index
    anova_results.append(anova_table.reset_index(drop=True))

# Combine results into a single DataFram
final_anova_df = pd.concat(anova_results, ignore_index=True)





# Create a copy of the original ANOVA results to preserve the source
df_anova = final_anova_df.copy()

# Define a mapping from raw ANOVA variable terms to simplified labels
label_map = {
    'C(Group)': 'Gr',
    'C(Sex)': 'Sex',
    'C(Year)': 'Year',
    'C(Group):C(Year)': 'Gr/Y',
    'C(Sex):C(Year)': 'Sex/Y'
}

# Filter the DataFrame to retain only the terms of interest
df_anova = df_anova[df_anova['Variable'].isin(label_map.keys())]

# Create a new column for simplified effect labels
df_anova['Effect'] = df_anova['Variable'].map(label_map)

# Format the F and p-values into a readable string
df_anova['F(p)'] = df_anova.apply(
    lambda row: f"{row['F']:.2f} (p < {row['PR(>F)']:.4f})", axis=1
)

# Pivot the table so each effect is a column and each feature is a row
table_formatted = df_anova.pivot(index='Feature', columns='Effect', values='F(p)')

# Reorder columns to match desired output
ordered_cols = ['Gr', 'Sex', 'Year', 'Gr/Y', 'Sex/Y']
table_formatted = table_formatted.reindex(columns=ordered_cols)
display(table_formatted)



