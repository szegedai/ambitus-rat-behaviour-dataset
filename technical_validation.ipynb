{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f8c0a1-dfe4-4cc3-9ba8-091973592b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data handling, visualization, and analysis\n",
    "import pandas as pd # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt # For creating static visualizations\n",
    "import seaborn as sns # For enhanced statistical data visualization\n",
    "import numpy as np # For numerical operations\n",
    "\n",
    "# Import statistical tools\n",
    "from scipy.stats import spearmanr # For computing Spearman rank correlation\n",
    "from statsmodels.stats.multitest import multipletests # For multiple testing correction (e.g., FDR)\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import scikit-learn tool for handling missing values\n",
    "from sklearn.impute import SimpleImputer # For imputing missing values\n",
    "\n",
    "# System and warning configuration\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # Prevents conflicts in certain parallel computing environments (e.g., on MacOS or when using MKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a915b2-2f80-428d-923c-06740b3ea4ba",
   "metadata": {},
   "source": [
    "### Load and Prepare Dataset\n",
    "\n",
    "In this step, we load the raw behavioral dataset from a .parquet file into a DataFrame. We also standardize gender-related columns (GR_Gender and Gender) by converting all values to lowercase, ensuring consistent formatting for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a940c2e-7e9f-4240-8fb9-4c08f26b2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the raw data file (Parquet format)\n",
    "file_path = \"raw/ambitus_0_15_log_24_07_2025.parquet\"\n",
    "# load the data into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "# Standardize gender-related columns by converting text to lowercase\n",
    "df['GR_Gender'] = df['GR_Gender'].str.lower()\n",
    "df['Gender'] = df['Gender'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dc604-db65-46a5-9d2e-a339118b99ff",
   "metadata": {},
   "source": [
    "### SimpleImputer example\n",
    "\n",
    "To ensure robust statistical analysis, missing values represented by -1 were first converted to NaN. We then applied SimpleImputer from scikit-learn using the mean imputation strategy on numeric columns that are not fully missing. This ensures each feature can be utilized without being dropped due to incomplete data.\n",
    "\n",
    "-> Playground cell <-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f22e5a7-3155-4362-8785-4aabada06e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace placeholder values (-1) with actual NaNs for proper missing value handling\n",
    "df_SI = df.replace(-1, np.nan)\n",
    "# Select all numeric columns from the DataFrame\n",
    "numeric_cols = df_SI.select_dtypes(include=\"number\").columns\n",
    "# Keep only numeric columns that are not entirely NaN\n",
    "numeric_cols = numeric_cols[~df_SI[numeric_cols].isna().all()]\n",
    "\n",
    "# Initialize a SimpleImputer with the mean strategy\n",
    "# (You can also use strategies like 'median', 'most_frequent', etc.) \n",
    "imputer = SimpleImputer(strategy=\"mean\")   # or 'median', 'most_frequent', etc.\n",
    "# Apply the imputer to fill missing values in numeric columns\n",
    "imputed_array = imputer.fit_transform(df_SI[numeric_cols])\n",
    "# Reconstruct a DataFrame from the imputed array with original indices and column names\n",
    "df_imputed = pd.DataFrame(imputed_array,\n",
    "                          columns=numeric_cols,\n",
    "                          index=df_SI.index)\n",
    "# Update the original DataFrame with the imputed values\n",
    "df_SI.loc[:, numeric_cols] = df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b92666d-09a1-4ae3-ab91-7ae0f4e796c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animal</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Season</th>\n",
       "      <th>Separation</th>\n",
       "      <th>G_S</th>\n",
       "      <th>Paradigm</th>\n",
       "      <th>Date_Ambitus</th>\n",
       "      <th>Year</th>\n",
       "      <th>NR</th>\n",
       "      <th>Group</th>\n",
       "      <th>...</th>\n",
       "      <th>Expl_E_BEF_Loco_ratio</th>\n",
       "      <th>Expl_I_BEF_Loco_ratio</th>\n",
       "      <th>Expl_EI_BEF_Loco_ratio</th>\n",
       "      <th>Expl_E_TOT_Loco_ratio</th>\n",
       "      <th>Expl_I_TOT_Loco_ratio</th>\n",
       "      <th>Expl_E_I_TOT_Loco_ratio</th>\n",
       "      <th>Eff_Expl_E</th>\n",
       "      <th>Eff_Expl_I</th>\n",
       "      <th>Eff_Expl_EI</th>\n",
       "      <th>E_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LE2F1_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>0_Autumn</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lisket</td>\n",
       "      <td>...</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>2.923077</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>2.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LE2F2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>0_Autumn</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lisket</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LE2F3_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>0_Autumn</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lisket</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.769231</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LE2F4_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>0_Autumn</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Lisket</td>\n",
       "      <td>...</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LE2F1_2</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>0_Autumn</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Lisket</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>Rat28</td>\n",
       "      <td>15</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>15_Winter</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>6</td>\n",
       "      <td>98</td>\n",
       "      <td>LE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>1.343750</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>Rat11</td>\n",
       "      <td>15</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>15_Winter</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>LE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>Rat12</td>\n",
       "      <td>15</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>15_Winter</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>LE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.653846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>Rat33</td>\n",
       "      <td>15</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>15_Winter</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>LE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.771429</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>Rat34</td>\n",
       "      <td>15</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>15_Winter</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>LE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>1.558824</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5368 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Animal  Generation  Season Separation        G_S  Paradigm  \\\n",
       "0     LE2F1_1           0  Autumn 2019-08-26   0_Autumn         1   \n",
       "1     LE2F2_1           0  Autumn 2019-08-26   0_Autumn         1   \n",
       "2     LE2F3_1           0  Autumn 2019-08-26   0_Autumn         2   \n",
       "3     LE2F4_1           0  Autumn 2019-08-26   0_Autumn         2   \n",
       "4     LE2F1_2           0  Autumn 2019-08-26   0_Autumn         1   \n",
       "...       ...         ...     ...        ...        ...       ...   \n",
       "5363    Rat28          15  Winter 2024-12-30  15_Winter         2   \n",
       "5364    Rat11          15  Winter 2024-12-30  15_Winter         1   \n",
       "5365    Rat12          15  Winter 2024-12-30  15_Winter         1   \n",
       "5366    Rat33          15  Winter 2024-12-30  15_Winter         2   \n",
       "5367    Rat34          15  Winter 2024-12-30  15_Winter         2   \n",
       "\n",
       "     Date_Ambitus  Year  NR   Group  ... Expl_E_BEF_Loco_ratio  \\\n",
       "0      2019-10-07     0   1  Lisket  ...              1.846154   \n",
       "1      2019-10-07     0   1  Lisket  ...              1.571429   \n",
       "2      2019-10-07     0   1  Lisket  ...              1.000000   \n",
       "3      2019-10-07     0   1  Lisket  ...              1.666667   \n",
       "4      2019-10-07     0   2  Lisket  ...              1.333333   \n",
       "...           ...   ...  ..     ...  ...                   ...   \n",
       "5363   2025-02-18     6  98      LE  ...              0.900000   \n",
       "5364   2025-02-18     6  99      LE  ...              0.600000   \n",
       "5365   2025-02-18     6  99      LE  ...              1.200000   \n",
       "5366   2025-02-18     6  99      LE  ...              1.125000   \n",
       "5367   2025-02-18     6  99      LE  ...              1.000000   \n",
       "\n",
       "     Expl_I_BEF_Loco_ratio  Expl_EI_BEF_Loco_ratio  Expl_E_TOT_Loco_ratio  \\\n",
       "0                 1.076923                2.923077               1.846154   \n",
       "1                 1.857143                3.428571               1.500000   \n",
       "2                 1.800000                2.800000               0.769231   \n",
       "3                 3.166667                4.833333               1.857143   \n",
       "4                 0.916667                2.250000               1.333333   \n",
       "...                    ...                     ...                    ...   \n",
       "5363              1.600000                2.500000               0.968750   \n",
       "5364              0.520000                1.120000               0.600000   \n",
       "5365              1.200000                2.400000               0.846154   \n",
       "5366              1.750000                2.875000               0.971429   \n",
       "5367              2.090909                3.090909               0.764706   \n",
       "\n",
       "      Expl_I_TOT_Loco_ratio  Expl_E_I_TOT_Loco_ratio  Eff_Expl_E  Eff_Expl_I  \\\n",
       "0                  1.076923                 2.923077    1.000000        0.75   \n",
       "1                  2.100000                 3.600000    1.000000        1.00   \n",
       "2                  1.000000                 1.769231    0.689107        1.00   \n",
       "3                  2.357143                 4.214286    0.689107        1.00   \n",
       "4                  0.916667                 2.250000    1.000000        1.00   \n",
       "...                     ...                      ...         ...         ...   \n",
       "5363               1.343750                 2.312500    0.689107        1.00   \n",
       "5364               0.520000                 1.120000    0.875000        1.00   \n",
       "5365               0.807692                 1.653846    1.000000        1.00   \n",
       "5366               0.800000                 1.771429    0.689107        1.00   \n",
       "5367               0.794118                 1.558824    0.689107        1.00   \n",
       "\n",
       "      Eff_Expl_EI     E_E  \n",
       "0        0.875000  0.8750  \n",
       "1        1.000000  1.0000  \n",
       "2        0.533333  1.0000  \n",
       "3        0.500000  1.0000  \n",
       "4        1.000000  1.0000  \n",
       "...           ...     ...  \n",
       "5363     0.533333  1.0000  \n",
       "5364     0.937500  0.9375  \n",
       "5365     1.000000  1.0000  \n",
       "5366     0.533333  1.0000  \n",
       "5367     0.533333  1.0000  \n",
       "\n",
       "[5368 rows x 104 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_SI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db065a5b-ff4e-466a-b3a5-df7b8934b1bd",
   "metadata": {},
   "source": [
    "### Feature-Wise Missing Data Summary\n",
    "\n",
    "To quantify the extent of missing data, we calculate the percentage of missing values for each feature across the entire dataset. This summary is exported as feature_missingness.csv, which is intended for inclusion as Supplementary Table S1 in the manuscript. It provides a transparent overview of data quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9b176-ae06-4c25-b8c3-bfd64e5d5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column in the full dataset\n",
    "missing_pct = df.isna().mean().mul(100).sort_values(ascending=False)\n",
    "\n",
    "# Export the missingness summary to a CSV file (for Supplementary Table S1)\n",
    "missing_pct.to_csv(\"results/feature_missingness.csv\",\n",
    "                   index_label=\"feature\",\n",
    "                   header=[\"percent_missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58af73-6a12-4cdb-ad8d-91f41a0e95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(missing_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424743e-8dc6-4508-a443-ca183f5587d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494b4aa-064b-4eec-ba7a-9c0a79795a63",
   "metadata": {},
   "source": [
    "### Visualizing the Top 30 Most Incomplete Features\n",
    "\n",
    "To identify the least complete features in the dataset, we visualize the top 30 variables with the highest proportion of missing values. The horizontal bar chart provides an intuitive overview of the data quality, highlighting features that may require imputation, exclusion, or careful interpretation. The plot is saved as a high-resolution PDF for inclusion in reports or supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea212da5-f8a4-468c-bf2d-3a36f12b6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 30 features with the highest proportion of missing values\n",
    "top30 = missing_pct.head(30)\n",
    "# Create a horizontal bar plot showing the missingness of these top 30 features\n",
    "plt.figure(figsize=(7, 9))\n",
    "top30[::-1].plot.barh(color=\"steelblue\") # Plot in reverse order for better readability\n",
    "# Label the axes and add a title\n",
    "plt.xlabel(\"Missing values (%)\")\n",
    "plt.title(\"Top 30 features by missing rate\")\n",
    "plt.tight_layout()\n",
    "# Adjust layout and save the figure as a high-resolution PDF\n",
    "plt.savefig(\"results/fig_missing_bar.pdf\", dpi=300)\n",
    "# Display the plot\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1727a3-65c4-44bb-91a5-87c3c84ed646",
   "metadata": {},
   "source": [
    "### Heatmap of Missing Data by Generation\n",
    "\n",
    "To explore whether missingness patterns vary across experimental generations, we computed the percentage of missing values for the top 30 features within each generation. The resulting heatmap highlights generation-specific gaps in the data, which may reflect differences in experimental design, data collection protocols, or recording errors. This visualization can guide decisions about feature selection, imputation strategies, or data quality controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894a29c-d1dd-40aa-a3c7-be0ade926e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table: rows = features, columns = generations\n",
    "# For the top 30 features, compute the percentage of missing values per generation\n",
    "miss_gen = (\n",
    "    df.loc[:,list(top30.index)+ [\"Generation\"]].groupby(\"Generation\")\n",
    "      .apply(lambda x: x.isna().mean())          # Calculate missing ratio for each feature\n",
    "      .T                                         # Transpose to make features as rows\n",
    "      .mul(100)                                  # Convert to percentage\n",
    ")\n",
    "# Plot a heatmap of missing values across generations for top 30 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    miss_gen,\n",
    "    cmap=\"viridis\",\n",
    "    vmin=0, vmax=100,\n",
    "    cbar_kws={\"label\": \"Missing (%)\"},\n",
    "    linewidths=0.2\n",
    ")\n",
    "# Add labels and formatting\n",
    "plt.title(\"Missing-value heatmap by generation(Top 30 features by missing rate)\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "# Save the figure as high-resolution PDF\n",
    "plt.savefig(\"results/fig_missing_heatmap.pdf\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a47c0b-64ee-4ed7-a589-2eac7f6f680b",
   "metadata": {},
   "source": [
    "### Spearman Correlation Among Top Behavioral Features\n",
    "\n",
    "To examine relationships between key behavioral readouts, we first excluded metadata and categorical variables (e.g., IDs, sex, group labels), then filtered the dataset to retain only numeric features with sufficient variance. From this cleaned subset, we selected the 30 most variable features and computed a Spearman correlation matrix to capture monotonic relationships between them. The heatmap provides insight into potential feature clusters, redundancies, or latent behavioral dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c677d-6994-4392-b82b-7cc709e78a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of keywords related to identifiers and non-behavioral metadata\n",
    "exclude_keywords = ['id', 'group', 'sex', 'trial', 'year', 'gen', 'rat', 'animal']\n",
    "# Identify columns to exclude if they contain any of the keywords\n",
    "exclude_columns = [col for col in df.columns if any(key in col.lower() for key in exclude_keywords)]\n",
    "# Drop the identified non-behavioral columns from the dataset\n",
    "df_filtered = df.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "# Keep only numeric columns\n",
    "df_numeric = df_filtered.select_dtypes(include='number')\n",
    "\n",
    "# Drop columns with constant values or near-zero variance\n",
    "df_numeric = df_numeric.loc[:, df_numeric.std() > 0.01]\n",
    "\n",
    "# Select top 30 variables by standard deviation\n",
    "top30_columns = df_numeric.std().sort_values(ascending=False).head(30).index\n",
    "df_top30 = df_numeric[top30_columns]\n",
    "\n",
    "# Plot the Spearman correlation matrix for the top 30 features\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_top30.corr(method=\"spearman\"), cmap=\"coolwarm\", center=0, annot=False, fmt=\".2f\", linewidths=1)\n",
    "plt.title(\"Correlation Matrix – Top 30 Behavioral Features\", fontsize=16)\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72667f9-6992-4a76-bd79-12468bdc711b",
   "metadata": {},
   "source": [
    "### Generational Drift Analysis\n",
    "\n",
    "To identify behavioral features that change systematically across generations, we computed Spearman correlations between each numeric feature and the generation number. This non-parametric approach captures monotonic trends regardless of linearity. After excluding non-numeric columns, we applied False Discovery Rate (FDR) correction to adjust for multiple hypothesis testing. The resulting ranked list of features (saved as gen_drift_spearman.csv) highlights traits that may reflect generational drift, such as adaptation, learned behavior, or methodological differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1ae4f-0af1-4e23-b8f7-b08307da1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate metadata & numeric fesature matrix\n",
    "meta_cols = [\"RatID\", \"Generation\"]                    # Add more metadata columns if needed\n",
    "all_feat_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "# Select only numeric and boolean columns (booleans are treated as numeric)\n",
    "numeric_feat_cols = df[all_feat_cols].select_dtypes(\n",
    "    include=[\"number\", \"boolean\"]            # bool is OK\n",
    ").columns.tolist()\n",
    "\n",
    "# Optionally: print a few of the excluded (non-numeric) columns\n",
    "dropped = set(all_feat_cols) - set(numeric_feat_cols)\n",
    "if dropped:\n",
    "    print(f\"{len(dropped)} non-numeric columns skipped: {sorted(dropped)[:5]} ...\")\n",
    "\n",
    "# Compute Spearman correlation between Generation and each numeric feature\n",
    "results = []\n",
    "gen = df[\"Generation\"].astype(\"float\") \n",
    "for f in numeric_feat_cols:\n",
    "    rho, p = spearmanr(gen, df[f], nan_policy=\"omit\")\n",
    "    results.append({\"feature\": f, \"rho\": rho, \"p\": p})\n",
    "# Compile results and sort by absolute correlation\n",
    "res = pd.DataFrame(results).sort_values(\"rho\", key=np.abs, ascending=False)\n",
    "# Apply FDR correction (Benjamini–Hochberg) for multiple testing\n",
    "res[\"q\"] = multipletests(res[\"p\"], method=\"fdr_bh\")[1]\n",
    "# Save results to CSV\n",
    "res.to_csv(\"results/gen_drift_spearman.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28731d3-fe78-4ec4-9db0-ddb60cb46fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1a0c2-3cde-49d7-aea5-ada0f6da8a0f",
   "metadata": {},
   "source": [
    "### Generational Drift – Correlation Heatmap of Top 25 Features\n",
    "\n",
    "To further investigate the structure of generational drift, we selected the top 25 features with the strongest correlations to generation. By aggregating feature values at the generation level (using medians), we reduced individual variability and highlighted population-level trends. A Spearman correlation heatmap reveals how these features co-vary across generations, suggesting potential latent dimensions or coordinated behavioral shifts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260b563-a421-487c-b590-f119db466f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 25 features showing the strongest generational drift\n",
    "top25 = res.head(25)[\"feature\"]\n",
    "# Compute the Spearman correlation matrix between these top 25 features\n",
    "# using their median values per generation (to smooth out noise)\n",
    "corr_sub = (df.groupby(\"Generation\")[top25]\n",
    "              .median()\n",
    "              .corr(method=\"spearman\")          # feature × feature, quick normalisation\n",
    "              .loc[top25, top25])\n",
    "\n",
    "# Plot the absolute Spearman correlations as a heatmap\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(np.abs(corr_sub), cmap=\"crest\", vmin=0, vmax=1,\n",
    "            cbar_kws={\"label\": \"abs(Spearman ρ)\"})\n",
    "# Add title and formatting\n",
    "plt.title(\"Generational drift – top 25 features\")\n",
    "plt.tight_layout()\n",
    "# Save the figure as a high-resolution PDF\n",
    "plt.savefig(\"results/fig_drift_heatmap.pdf\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f8408-f094-4950-aad9-8030349fa4a4",
   "metadata": {},
   "source": [
    "### Generational Trends of Key Behavioral Metrics\n",
    "\n",
    "To illustrate how specific behavioral traits evolve across generations, we plotted three representative metrics using a dual-axis line chart. The left y-axis shows latency (in milliseconds), while the right y-axis displays count-based and ratio-based metrics. Median values and interquartile ranges (25th to 75th percentiles) are visualized to highlight both central tendency and variability. This plot captures long-term trends that may reflect generational adaptation or procedural shifts in the experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a836ea2-d5cc-4404-afbf-c614acda0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dual-axis plot to visualize multiple metrics with different scales\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4.5))\n",
    "ax2 = ax1.twinx() # Secondary y-axis\n",
    "\n",
    "# Assign consistent colors using Matplotlib's default color cycle\n",
    "col_map = dict(zip(features_to_plot, [\"C0\", \"C1\", \"C2\"]))\n",
    "\n",
    "# LEFT y-axis: latency feature (typically larger values)\n",
    "f_left = \"Expl_E_AFT_T\"\n",
    "ax1.plot(df_line[\"Generation\"], df_line[f\"{f_left}_med\"],\n",
    "         label=f_left, color=col_map[f_left])\n",
    "ax1.fill_between(df_line[\"Generation\"],\n",
    "                 df_line[f\"{f_left}_q25\"],\n",
    "                 df_line[f\"{f_left}_q75\"],\n",
    "                 alpha=0.20, color=col_map[f_left])\n",
    "\n",
    "ax1.set_ylabel(\"Latency (ms)\")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "\n",
    "# RIGHT y-axis: count and ratio features (typically smaller scale)\n",
    "for f in [\"Expl_E_AFT_Nr\", \"Expl_E_TOT_Loco_ratio\"]:\n",
    "    ax2.plot(df_line[\"Generation\"], df_line[f\"{f}_med\"],\n",
    "             label=f, color=col_map[f])\n",
    "    ax2.fill_between(df_line[\"Generation\"],\n",
    "                     df_line[f\"{f}_q25\"],\n",
    "                     df_line[f\"{f}_q75\"],\n",
    "                     alpha=0.20, color=col_map[f])\n",
    "\n",
    "ax2.set_ylabel(\"Count / Ratio\")\n",
    "\n",
    "# Combine legends from both axes\n",
    "handles, labels = [], []\n",
    "for ax in (ax1, ax2):\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    handles.extend(h)\n",
    "    labels.extend(l)\n",
    "\n",
    "ax1.legend(handles, labels, frameon=False, loc=\"upper left\")\n",
    "# Add plot title and finalize layout\n",
    "ax1.set_title(\"Generational trend of key behavioural metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/fig_drift_lineplot.pdf\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e877839-8aff-4153-b15e-6ea15f1b0c79",
   "metadata": {},
   "source": [
    "### Factorial ANOVA – Group × Sex × Year Effects\n",
    "\n",
    "To evaluate whether behavioral outcomes differ across experimental groups, sexes, and years, we performed a factorial ANOVA on six key behavioral features. The model includes:\n",
    "- Main effects: Group, Sex, Year\n",
    "- Two-way interactions: Group × Year and Sex × Year\n",
    "  \n",
    "This analysis identifies which behavioral measures show statistically significant differences across subpopulations or temporal contexts. The results are compiled into a unified ANOVA table (final_anova_df) for further inspection and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d47a2-7474-4335-9256-2e9b319ff3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where grouping variables are missing\n",
    "df = df.dropna(subset=['GR_Gender', 'Year'])\n",
    "# Normalize gender information (lowercase for consistency)\n",
    "df['Sex'] = df['Gender'].str.lower()\n",
    "df['Group_Sex'] = df['GR_Gender'].str.lower()\n",
    "\n",
    "# Define variables to test and corresponding feature names in the dataset\n",
    "anova_targets = {\n",
    "    'LOCO_TOT': 'Locomotion (Loco_TOT)',\n",
    "    'LOCO_BEF': 'Locomotion frequency (LOCO_BEF)',\n",
    "    'EXPL_TOT': 'Exploration (Expl_TOT)',\n",
    "    'Expl_E_I_BEF_Nr': 'Exploration frequency (Expl_BEF)',\n",
    "    'L_C': 'Learning capacity (L_C)',\n",
    "    'E_E': 'Effective exploration ratio (E_E)'\n",
    "}\n",
    "\n",
    "# Initialize a list to collect ANOVA results\n",
    "anova_results = []\n",
    "\n",
    "# Perform two-way factorial ANOVA for each target variable\n",
    "# Including main effects and interactions with Year\n",
    "for feature, description in anova_targets.items():\n",
    "    formula = f\"{feature} ~ C(Group) + C(Sex) + C(Year) + C(Group):C(Year) + C(Sex):C(Year)\"\n",
    "    model = ols(formula, data=df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    # Annotate results with feature description and variable name\n",
    "    anova_table[\"Feature\"] = description\n",
    "    anova_table[\"Variable\"] = anova_table.index\n",
    "    anova_results.append(anova_table.reset_index(drop=True))\n",
    "\n",
    "# Combine results into a single DataFram\n",
    "final_anova_df = pd.concat(anova_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c0998-69d5-4932-b2c1-f27b83dc810e",
   "metadata": {},
   "source": [
    "### ANOVA Summary Table (Formatted for Readability)\n",
    "\n",
    "The raw factorial ANOVA results are reformatted into a clean summary table. Only the main effects and relevant two-way interactions are retained:\n",
    "- Gr: Experimental Group\n",
    "- Sex: Biological Sex\n",
    "- Year: Testing Year\n",
    "- Gr/Y: Group × Year interaction\n",
    "- Sex/Y: Sex × Year interaction\n",
    "\n",
    "Each cell shows the F-statistic along with the associated p-value, helping to quickly identify which variables are significantly influenced by group, sex, year, or their interactions. This format is well-suited for publication or supplementary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8288f-9a67-4f1c-9151-3dd7fefed97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original ANOVA results to preserve the source\n",
    "df_anova = final_anova_df.copy()\n",
    "\n",
    "# Define a mapping from raw ANOVA variable terms to simplified labels\n",
    "label_map = {\n",
    "    'C(Group)': 'Gr',\n",
    "    'C(Sex)': 'Sex',\n",
    "    'C(Year)': 'Year',\n",
    "    'C(Group):C(Year)': 'Gr/Y',\n",
    "    'C(Sex):C(Year)': 'Sex/Y'\n",
    "}\n",
    "\n",
    "# Filter the DataFrame to retain only the terms of interest\n",
    "df_anova = df_anova[df_anova['Variable'].isin(label_map.keys())]\n",
    "\n",
    "# Create a new column for simplified effect labels\n",
    "df_anova['Effect'] = df_anova['Variable'].map(label_map)\n",
    "\n",
    "# Format the F and p-values into a readable string\n",
    "df_anova['F(p)'] = df_anova.apply(\n",
    "    lambda row: f\"{row['F']:.2f} (p < {row['PR(>F)']:.4f})\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot the table so each effect is a column and each feature is a row\n",
    "table_formatted = df_anova.pivot(index='Feature', columns='Effect', values='F(p)')\n",
    "\n",
    "# Reorder columns to match desired output\n",
    "ordered_cols = ['Gr', 'Sex', 'Year', 'Gr/Y', 'Sex/Y']\n",
    "table_formatted = table_formatted.reindex(columns=ordered_cols)\n",
    "display(table_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415ef60-2620-4606-bd7c-9d2c6cf2504d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
